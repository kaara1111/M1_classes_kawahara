第一回
#1:出力が意図をもってなされているように感じるときに賢いと思います．例えば，chat-gptのような対話形式の人工知能では，ユーザ側の発言に対して，会話がかみ合うような出力がなされたときです．昔の人工知能と比べ，会話がかみ合う可能性が高くなっているのであればその人工知能は昔のものよりも賢いと言えるような一つの基準になるかと思います．

#2:現場全体を統括する管理者や人工知能を作成する職業については代替されないと考えます．そのような職業についてまで完全にAIにしてしまうと，何かしらの事故が起き，AIが稼働しないというような不具合が起きた時に対応することができないためです．また，肉体労働のうち、柔軟な対応が必要となるようなものについても，実際に作業を行わせるマシンにかかるコストが人間を雇うよりも大きくなってしまう可能性があるため，代替されないのではないかと考えます．

第二回
#1:分類問題で教師あり学習．

#2:x_2=3*x_1/2-3/2

第三回
#1

1)g(x)=0.2+0.3x_1

2)-2/3

3)２つのクラスの識別関数の差を識別関数としているから．

#2

学習係数が大きいほどに一度の更新でwが動く距離が大きくなり，学習係数が小さいほど距離も小さくなる

#3

No.　学習データと未知データのデータ分布が異なっている場合や，そうでなくても外れ値が存在する場合には正しく分類できるとは限らない．

第四回
#1.

晴：I(2/5,3/5)=-2/5log(2/5)-3/5log(3/5)=-2/5-3/5log3+log5=-0.4-0.6*1.58+2.32=-1.348+2.32=0.972

曇：I(4/4,0/4)=-4/4log(4/4)-0log0=0

雨：I(2/5,3/5)=0.972

Iの期待値：5/14*0.972+4/14*0+5/14*0.972=0.694

#2.

左：湿度（晴の中で湿度が高ならば×，普通ならば○になっている．）

右：風（晴の中で有ならば×，無ならば○になっている．）

第五回

#1

1)

closed testの場合では学習データと評価データのデータ分布が等しいことが保証されるため．

2)

・Closed test も Open test もそこそこ高性能が得られた ：

Open testの場合の学習データと評価データのデータ分布が近い．

・Closed testの性能は高いが，Open test ではとても低い ：

Open testの場合の学習データと評価データのデータ分布が遠い．過学習が起こっている．

・Closed test の性能が（Open testの性能も）とても低い ：

学習データのデータ数が少なすぎる等，十分な学習ができない．

#2

・ドメイン：発話全体

・意図：発話全体

・スロット値：単語

等しい．

第六回
#1

履歴に関するバックプロパゲーション(BPTT)によって単語間の関係を表そうとしている．

文が長くなると単語の位置についての距離が大きくなってしまうため学習が難しい．

第七回
#1

・物理的な定義：音響信号の振幅が一定値以下である区間が一定の長さ以上ある場合発話の区切り目とする．

・対話行為による定義：意味的なまとまりを「一発話」とみなす 

・話者交替による定義：相手の発話に区切られた区間を一発話とみなす 

話者交替による定義は話者の交代を発話の区切り目としているので音声対話システムで実際にユーザと話すときに使用できない．

#2

少なくとも人工知能は人ではないので責任を負うことはできない．その他の場合であればケースによって責任を負う人間を司法によって決めるしかないと考える．例えば，車の所有者や運転者が危険な改造等を行なっていた場合はその人に，プログラムに問題があるのであればメーカーが責任を負うべきである．

第八回
#1

・物理的な定義：音響信号の振幅が一定値以下である区間が一定の長さ以上ある場合発話の区切り目とする．

・対話行為による定義：意味的なまとまりを「一発話」とみなす 

・話者交替による定義：相手の発話に区切られた区間を一発話とみなす 

話者交替による定義は話者の交代を発話の区切り目としているので音声対話システムで実際にユーザと話すときに使用できない．

#2

少なくとも人工知能は人ではないので責任を負うことはできない．その他の場合であればケースによって責任を負う人間を司法によって決めるしかないと考える．例えば，車の所有者や運転者が危険な改造等を行なっていた場合はその人に，プログラムに問題があるのであればメーカーが責任を負うべきである．

第九回
#1.

5 * 48000 = 240000点

第十回
#1.

二信号の時刻の差が小さいほどに値は大きく，大きいほどに値は小さくなる．

#2.

w0 = 0とすると，

w1 = 2\mu e1 u1 = 2\mu d1 u1 =2\mu h^T u1 u1となる．

u1の次数は2であるため，uの係数の二乗に比例した更新式となる．

第十一回
＃１

P(s_(t+1)=A) = 0.3*0.9 + 0.7*0.6 = 0.27 + 0.42 = 0.69

P(s_(t+1)=B) = 0.3*0.1 + 0.7*0.4 = 0.03 + 0.28 = 0.31

第十二回
＃１

Aはbに，Bはaに対応する．

#2

時間方向に特徴的なパターンがある場合には学習精度が低下し，ない場合には向上する．

第十三回
＃１

人が話している内容（日常会話・ビジネストークなど）によって予想する語の確率を変える．

＃２

②

第十四回
#1

的の個数個のクラスタでクラスタリングを行なう．

各クラスタ内の平均座標を的の座標の推定値，分散をばらつきとする．

教師なし学習

#2

パラメータ推定に用いたテキストと同じ分野の音声認識であれば，単語の出現確率が等しいはずなので高い有用性が得られると考えられる．

逆に，推定に用いたテキストと違う分野であれば，有用性は低くなる．他の分野では「よくある」意味で用いられている言葉が，特定の意味で使われるような場合であれば，有用性はさらに低くなる可能性があると考えられる．

第十五回
＃１

/e/：0.4*0.7*0.4*0.3*0.3*0.4 + 0.4*0.3*0.3*0.6*0.3*0.4 = 0.004032+0.002592 = 0.006624

/o/：0.7*0.6*0.7*0.4*0.4*0.8 + 0.7*0.4*0.6*0.2*0.4*0.8 = 0.037632+0.010752 = 0.048384

よって/o/．